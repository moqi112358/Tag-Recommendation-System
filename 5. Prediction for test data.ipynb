{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "874f6b9dbd499c23fc88dbf585ede9e21ec1f982"
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd\nimport os\nimport torch.utils.data as data\nimport torch\nimport gensim \nimport pickle\nfrom os import listdir\nfrom os.path import join\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.autograd import Variable\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\nargs = {}\nfile_id = '1'\nargs['start_step'] = (int(file_id) - 1) *22500\nargs['end_step'] = int(file_id) * 22500\nargs['use_TextCNN'] = True\nargs['use_TextRCNN'] = True\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"./\"))\n\ntorch.cuda.is_available()",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['fork-of-rcnn-model-4', 'tag-recommendation-system-list', 'train-validation-data-split', 'train-textcnn-best-model', 'hackerearth-deep-learning-challenge-4']\n['.ipynb_checkpoints', '__notebook_source__.ipynb']\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0c9a0ffde7300fb90b532e24f22a09149f2290d"
      },
      "cell_type": "code",
      "source": "def test_read_input(fold_dir):\n    raw_data = pd.read_csv(fold_dir+'test.csv')\n    logging.info(\"reading raw data...this may take a while\")\n    l = len(raw_data)\n    article_list = []\n    title_list = []\n    id_list = []\n    for i in range(l):\n        line = raw_data['article'].iloc[i]\n        title = raw_data['title'].iloc[i]\n        id = raw_data['id'].iloc[i]\n        if (i % 10000 == 0):\n            logging.info(\"read {0} reviews\".format(i))\n        # do some pre-processing and return list of words for each article\n        line = gensim.utils.simple_preprocess(line)\n        title = gensim.utils.simple_preprocess(title)\n        article_list.append(line)\n        title_list.append(title)\n        id_list.append(id)\n    return id_list, title_list, article_list\n\ntest_id_list, test_title_list, test_article_list = test_read_input('../input/hackerearth-deep-learning-challenge-4/')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def read_vocab(fold_dir):        \n    f = open(fold_dir + 'label_vocab.pickle','rb')\n    args['label_vocab'] = pickle.load(f)\n    f.close()\n    f = open(fold_dir + 'title_vocab.pickle','rb')\n    args['title_vocab'] = pickle.load(f)\n    f.close()\n    f = open(fold_dir + 'article_vocab.pickle','rb')\n    args['article_vocab'] = pickle.load(f)\n    args['label_dim'] = len(args['label_vocab'])\n    f.close()\n    f = open(fold_dir + 'unknow_word_embedding.pickle','rb')\n    args['unknow_for_article_embedding'],  args['unknow_for_title_embedding'] = pickle.load(f)\n    f.close()\n    return args\n\nargs = read_vocab('../input/tag-recommendation-system-list/')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e0adc9074cc95d7f24cd76d0551756d2985ff17"
      },
      "cell_type": "code",
      "source": "class Alphabet(dict):\n    def __init__(self, start_id=1):\n        self.fid = start_id\n    def add(self, item):\n        idx = self.get(item, None)\n        if idx is None:\n            idx = self.fid\n            self[item] = idx\n            self.fid += 1\n        return idx\n    def dump(self, fname):\n        with open(fname, 'w') as out:\n            for k in sorted(self.keys()):\n                out.write(\"{}\\t{}\\n\".format(k, self[k]))\n\ndef parse_embed(embed_dict,unknow_word):\n    alphabet, embed_mat = Alphabet(), []\n    vocab_size, embed_dim = len(embed_dict), len(embed_dict[list(embed_dict.keys())[0]])\n    unknown_word_idx = 0\n    embed_mat.append(unknow_word)\n    for word in embed_dict:\n        alphabet.add(word)\n        embed_mat.append(embed_dict[word])\n    dummy_word_idx = alphabet.fid\n    embed_mat.append(np.zeros(embed_dim))\n    return alphabet, embed_mat, unknown_word_idx, dummy_word_idx\n\ntest_title_alphabet, test_title_embed_mat, test_unknown_title_idx, test_dummy_title_idx = parse_embed(args['title_vocab'],args['unknow_for_title_embedding'])\ntest_article_alphabet, test_article_embed_mat, test_unknown_article_idx, test_dummy_article_idx = parse_embed(args['article_vocab'], args['unknow_for_article_embedding'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d3d28959ad0a46b02e0a251a335ab33a91c7050"
      },
      "cell_type": "code",
      "source": "len_aritle = 500 # mean 176.22896799025133\nlen_title = 15 # mean 8.178064471860113\n\ndef pad_list(alphabet, embed_mat, unknown_idx, dummy_idx, list_file, length):\n    result = []\n    for l in list_file:\n        if len(l) > length:\n            l = l[len(l) - length:]\n        else:\n            l = [0] * (length - len(l)) + l\n        tmp = []\n        for i in l:\n            if i == 0:\n                tmp.append(embed_mat[dummy_idx])\n            elif i not in alphabet:\n                tmp.append(embed_mat[unknown_idx])\n            elif i in alphabet:\n                tmp.append(embed_mat[alphabet[i]])\n        result.append(tmp)\n    return result\n\nargs['test_title_embedding_pad'] = pad_list(test_title_alphabet, test_title_embed_mat, test_unknown_title_idx, test_dummy_title_idx, test_title_list, len_title)\n\ndel test_title_alphabet, test_title_embed_mat, test_unknown_title_idx, test_dummy_title_idx, test_title_list\n\nargs['test_article_embedding_pad'] = pad_list(test_article_alphabet, test_article_embed_mat, test_unknown_article_idx, test_dummy_article_idx, test_article_list, len_aritle)\n\ndel test_article_alphabet, test_article_embed_mat, test_unknown_article_idx, test_dummy_article_idx, test_article_list",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1dd581ae8373acbdf83304c944effabea5beadb"
      },
      "cell_type": "code",
      "source": "class TestDatasetFromFile(data.Dataset):\n    def __init__(self, args, input_transform=True, start = 0, end = 0):\n        super(TestDatasetFromFile, self).__init__()\n        self.test_article_embedding_data = args['test_article_embedding_pad'][start:end]\n        self.test_title_embedding_data = args['test_title_embedding_pad'][start:end]\n        self.label_vocab = args['label_vocab']\n        self.input_transform = input_transform\n\n    # 在__getitem__中加载图片，并且将传入的transformation操作运用到\n    # 加载的图片中。 `input = self.input_transforms(input)`\n    # 这里的 self.input_transforms就是传入的\"类的实例\"，由于类是callable的\n    # 所以可以 \"类的实例（参数）\"这样调用。在上一篇博客说到了这个。\n    def __getitem__(self, index):\n        article_input = self.test_article_embedding_data[index]\n        title_input = self.test_title_embedding_data[index]\n        if self.input_transform:\n            input = [torch.FloatTensor(article_input),torch.FloatTensor(title_input)]\n        else:\n            input = torch.FloatTensor(article_input)\n        return input\n\n    def __len__(self):\n        return len(self.test_title_embedding_data)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10b3d7870533408c4cf556f30683125c1f4dcdb8"
      },
      "cell_type": "code",
      "source": "class TimeDistributed(nn.Module):\n    def __init__(self, module):\n        super(TimeDistributed, self).__init__()\n        self.module = module\n\n    def forward(self, x):\n        if len(x.size()) <= 2:\n            return self.module(x)\n        n, t = x.size(0), x.size(1)\n        # merge batch and seq dimensions\n        x_reshape = x.contiguous().view(t * n, x.size(2))\n        y = self.module(x_reshape)\n        # we have to reshape Y\n        y = y.contiguous().view(n, t, y.size()[1])\n        return y\n\nclass K_MaxPooling(nn.Module):\n    def __init__(self, k):\n        super(K_MaxPooling, self).__init__()\n        self.k = k\n        \n    def forward(self, x, dim):\n        index = x.topk(self.k, dim=dim)[1].sort(dim=dim)[0]\n        return x.gather(dim, index)\n\nclass textCNN(nn.Module):\n    def __init__(self,args):\n        super(textCNN, self).__init__()\n        article_dim = args['article_dim'] #500\n        title_dim = args['title_dim'] #15\n        label_dim = args['label_dim'] # 36321\n        train_size = args['train_size']\n        embed_dim = args['embed_dim'] # 300\n        # self.embeding = nn.Embedding(vocb_size, dim,_weight=embedding_matrix)\n\n        D = embed_dim\n        C = label_dim\n        Ci = 1\n        Co = 300#opt['kernel_num']\n        #Ks = opt['kernel_sizes']\n        Ks1 = [1,2,3,4,5]\n        Ks2 = [3,4,5,6,7]\n        \n        self.tdfc1 = nn.Linear(D, 300)#512)\n        self.td1 = TimeDistributed(self.tdfc1)\n        self.tdbn1 = nn.BatchNorm2d(1)\n        \n        self.tdfc2 = nn.Linear(D, 300)#512)\n        self.td2 = TimeDistributed(self.tdfc2)\n        self.tdbn2 = nn.BatchNorm2d(1)\n\n        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, 290)) for K in Ks1])\n        self.convbn1 = nn.ModuleList([nn.BatchNorm2d(Co) for i in range(len(Ks1))])\n        self.convs2 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, 290)) for K in Ks2])\n        self.convbn2 = nn.ModuleList([nn.BatchNorm2d(Co) for i in range(len(Ks2))])\n        \n        self.kmax_pooling = K_MaxPooling(5)\n    \n        self.fc1 = nn.Linear(33000, C)#4096)\n        # self.fc1 = nn.Linear(115200, C)#4096)\n        self.bn1 = nn.BatchNorm1d(C)#4096)\n        self.fc2 = nn.Linear(C, C)#4096, C)\n\n    def forward(self, x, y):\n        batch_size = x.size(0)\n        x = x.detach()\n        x = F.relu(self.tdbn1(self.td1(x).unsqueeze(1)))\n        y = y.detach()\n        y = F.relu(self.tdbn2(self.td2(y).unsqueeze(1)))\n        x = [F.relu(self.convbn1[i](conv(x))).squeeze(3) for i, conv in enumerate(self.convs1)]\n        #print(x)\n        # x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n        x = [self.kmax_pooling(i, 2).mean(2).squeeze(1) for i in x]\n        x = torch.cat(x, 1)\n        \n        y = [F.relu(self.convbn2[i](conv(y))).squeeze(3) for i, conv in enumerate(self.convs2)]\n        # y = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in y]\n        y = [self.kmax_pooling(i, 2).mean(2).squeeze(1) for i in y]\n        \n        y = torch.cat(y, 1)\n        x = torch.cat((x, y), 1)\n        x = x.view(batch_size, -1)\n        x = F.relu(self.bn1(self.fc1(x)))\n        logit = self.fc2(x)\n        return logit",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "575f7f1422f18092923461e912c1cce1e8014135"
      },
      "cell_type": "code",
      "source": "class RCNN(nn.Module):\n    def __init__(self,args):\n        super(RCNN, self).__init__()\n        article_dim = args['article_dim'] #500\n        title_dim = args['title_dim'] #15\n        label_dim = args['label_dim'] # 36321\n        train_size = args['train_size']\n        embed_dim = args['embed_dim'] # 300\n        # self.embeding = nn.Embedding(vocb_size, dim,_weight=embedding_matrix)\n        D = embed_dim\n        C = label_dim\n        self.num_layer = 1\n\n        self.tdfc1 = nn.Linear(D, 300)\n        self.td1 = TimeDistributed(self.tdfc1)\n        self.tdbn1 = nn.BatchNorm2d(1)\n\n        self.tdfc2 = nn.Linear(D, 300)\n        self.td2 = TimeDistributed(self.tdfc2)\n        self.tdbn2 = nn.BatchNorm2d(1)\n\n        self.lstm1 = nn.LSTM(input_size = 300, hidden_size = 300, num_layers = self.num_layer, batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(input_size = 300, hidden_size = 300, num_layers = self.num_layer, batch_first=True, bidirectional=True)\n\n        self.conv1 = nn.Conv2d(1, 1024 * 6, (3, 300+300+300))\n        # self.conv1 = nn.Conv2d(1, 512, (1, 256+256+D))\n        self.convbn1 = nn.BatchNorm2d(1024 * 6)\n        # self.convbn1 = nn.BatchNorm2d(512)\n        self.conv2 = nn.Conv2d(1, 1024 * 6, (3, 300+300+300))\n        # self.conv2 = nn.Conv2d(1, 512, (1, 256+256+D))\n        self.convbn2 = nn.BatchNorm2d(1024 * 6)\n        # self.convbn2 = nn.BatchNorm2d(512)\n        \n        self.fc1 = nn.Linear(1024 * 6 * 2, C)\n       \t# self.fc = nn.Linear(1024, C)\n        self.bn1 = nn.BatchNorm1d(C)#4096)\n        self.dropout = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(C, C)#4096, C)   \n        \n    def forward(self, x, y):\n        batch_size = x.size(0)\n        x = x.detach()\n        x = F.relu(self.tdbn1(self.td1(x).unsqueeze(1))).squeeze(1)\n        y = y.detach()\n        y = F.relu(self.tdbn2(self.td2(y).unsqueeze(1))).squeeze(1)\n\n        h0_1 = torch.randn(self.num_layer * 2, batch_size, 300)\n        c0_1 = torch.randn(self.num_layer * 2, batch_size, 300)\n        # h0_1 = Variable(torch.randn(2, batch_size, self.D))\n        # c0_1 = Variable(torch.randn(2, batch_size, self.D))\n        h0_1 = h0_1.cuda()\n        c0_1 = c0_1.cuda()\n        o1, _ = self.lstm1(x, (h0_1, c0_1)) # (h0_1, c0_1) \n        x = torch.cat((x, o1), 2)\n        x = F.relu(self.convbn1(self.conv1(x.unsqueeze(1))).squeeze(3))\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        \n        # y = self.get_context_embedding(y, 2)\n        h0_2 = Variable(torch.randn(self.num_layer * 2, batch_size, 300))\n        c0_2 = Variable(torch.randn(self.num_layer * 2, batch_size, 300))\n        # h0_2 = Variable(torch.randn(2, batch_size, self.D))\n        # c0_2 = Variable(torch.randn(2, batch_size, self.D))\n        h0_2 = h0_2.cuda()\n        c0_2 = c0_2.cuda()\n        o2, _ = self.lstm2(y, (h0_2, c0_2))\n        y = torch.cat((y, o2), 2)\n        \n        y = F.relu(self.convbn2(self.conv2(y.unsqueeze(1))).squeeze(3))\n        y = F.max_pool1d(y, y.size(2)).squeeze(2)\n        \n        x = torch.cat((x, y), 1)\n        x = x.view(batch_size, -1)\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = self.dropout(x)\n        logit = self.fc2(x)\n        \n        return logit",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6ebc42c5c0883811706fcf7c6e504bbd7fe7146"
      },
      "cell_type": "code",
      "source": "if args['use_TextCNN']:\n    args['TextCNN_model'] = torch.load('../input/train-textcnn-best-model/TextCNN_model.pkl',map_location=lambda storage, loc: storage)\n    args['TextCNN_model'].cuda()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3afc8213ccbb85da2562193194d58bb701bc8c1"
      },
      "cell_type": "code",
      "source": "if args['use_TextRCNN']:\n    args['TextRCNN_model'] = torch.load('../input/fork-of-rcnn-model-4/TextRCNN_model.pkl',map_location=lambda storage, loc: storage)\n    args['TextRCNN_model'].cuda()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c261317389b4328021d5f369d31ba6ad518eb27"
      },
      "cell_type": "code",
      "source": "test_dataset = torch.utils.data.DataLoader(\n         TestDatasetFromFile(args, input_transform=True, start = args['start_step'], end = args['end_step']), \n         batch_size= 15, shuffle= False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a58099fc6e08ac94a40017106214f981e063e6e6"
      },
      "cell_type": "code",
      "source": "def predict(args, test_dataset):\n    label_dim = args['label_dim']\n    if args['use_TextRCNN']:\n        args['TextRCNN_model'].eval()\n    if args['use_TextCNN']:\n        args['TextCNN_model'].eval()\n    \n    #predict_label_list_CNN = []\n    #predict_label_list_RCNN = []\n    predict_label_list_all = []\n    for step, data in enumerate(test_dataset):\n        if step % 500 == 0:\n            logging.info('Training step {} finished!'.format(step))\n        train_x = data\n        article_x, title_x = train_x[0], train_x[1]\n        article_x, title_x = Variable(article_x), Variable(title_x)\n        batch_size = article_x.size(0)\n        article_x, title_x = article_x.cuda(), title_x.cuda()\n        if args['use_TextRCNN'] and args['use_TextCNN']:\n            output1 = args['TextRCNN_model'](article_x, title_x)\n            output2 = args['TextCNN_model'](article_x, title_x)\n            output = output1 + output2\n        elif args['use_TextRCNN']:\n            output = args['TextRCNN_model'](article_x, title_x)\n        elif args['use_TextCNN']:\n            output = args['TextCNN_model'](article_x, title_x)\n        predict_label_list_all.extend([list(i) for i in np.array(output.cpu().data.numpy())])\n        #predict_label_list_RCNN.extend([list(i) for i in np.array(output.cpu().data.numpy())])\n        #predict_label_list_CNN.extend([list(i) for i in np.array(output.cpu().data.numpy())])\n    logging.info('Training step {} finished!'.format(step))\n    predict_label_list_all = np.array(predict_label_list_all)\n    # predict_label_list_RCNN = np.array(predict_label_list_RCNN)\n    # predict_label_list_CNN = np.array(predict_label_list_CNN)\n    return predict_label_list_all #, predict_label_list_RCNN, predict_label_list_CNN\n\ndef threshold(predict_label_list,args):\n    label_dim = args['label_dim']\n    predict_y = []\n    for observation in predict_label_list:\n        labeled = observation > 0\n        sum_labeled = sum(labeled)\n        if 0 < sum_labeled <= 5:\n            tmp = list(labeled)\n        elif sum_labeled > 5:\n            tmp = [0]*label_dim\n            for i in np.argpartition(observation, -5)[-5:]:\n                tmp[i] = 1\n        elif sum_labeled == 0:\n            tmp = [0]*label_dim\n            for i in np.argpartition(observation, -3)[-3:]:\n                tmp[i] = 1\n        predict_y.append(tmp)\n    predict_y = np.array(predict_y)\n    return predict_y\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5ea00d2bcea015d9c9ccd4a6b0c8e6066e04834"
      },
      "cell_type": "code",
      "source": "from datetime import datetime \na=datetime.now()\npredict_label_list_all = predict(args, test_dataset)\npred_y_all = threshold(predict_label_list_all,args)\n#pred_y_RCNN = threshold(predict_label_list_RCNN,args)\n#pred_y_CNN = threshold(predict_label_list_CNN,args)\nb=datetime.now()\n(b-a).seconds",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de4c6f86ad404d2c113a6e6e556fd02b7322209c"
      },
      "cell_type": "code",
      "source": "f = open('pred_y'+file_id+'_all.pickle','wb')\npickle.dump(pred_y_all,f)\nf.close()\n'''\nf = open('pred_y'+file_id+'_RCNN.pickle','wb')\npickle.dump(pred_y_RCNN,f)\nf.close()\nf = open('pred_y'+file_id+'_CNN.pickle','wb')\npickle.dump(pred_y_CNN,f)\nf.close()\n'''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2138538e21ee463013002ee20111ef26e8a36593"
      },
      "cell_type": "code",
      "source": "'''\ndef create_predict_file(args,test_id_list,pred_y):\n    args['id_to_label'] = {args['label_vocab'][i] : i for i in args['label_vocab']}\n    f = open('predict.csv','w')\n    f.write('id,tags\\n')\n    for line_id in range(len(pred_y)):\n        y = pred_y[line_id]\n        label_y = []\n        for i in range(len(y)):\n            if y[i] == 1:\n                label_y.append(args['id_to_label'][i])\n        label_str='|'.join(label_y)\n        f.write('%s,%s\\n' % (test_id_list[line_id], label_str))\n    f.close()\n    return\n\ncreate_predict_file(args,test_id_list,pred_y)\n'''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "638f4f4c7cab0cf3a6b61ddf1e93143869c9476b"
      },
      "cell_type": "code",
      "source": "#torch.save(model.cpu(), 'TextCNN_model.pkl')  # 保存整个网络\n# torch.save(model.state_dict(), 'TextCNN_model_para.pkl')   # 只保存网络中的参数 (速度快, 占内存少)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "512f9092391652a16446010ac95f1a0073cd3990"
      },
      "cell_type": "code",
      "source": "#torch.save(model.cpu().state_dict(), 'TextCNN_model_para.pkl')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5a3e9bdb798774ef3491b39b5c6b6e299c2b2a0"
      },
      "cell_type": "code",
      "source": "# TextCNN_model = torch.load('TextCNN_model.pkl',map_location=lambda storage, loc: storage)\n# TextCNN_model_para = textCNN(args)\n# TextCNN_model_para.load_state_dict(torch.load('TextCNN_model_para.pkl',map_location=lambda storage, loc: storage))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}